## 就业方向

### 岗位方向

* 智能对话
    * 智能客服：京东，淘宝在线客服；快递售后机器人；政企只能外呼；海底捞智能预约机器人；网上商城售后机器人
    * 智能音箱：小度音响；天猫精灵；小爱同学
    * 智能硬件：车载导航；siri

* 智能搜索
    * 搜索引擎搜索：百度搜索；知乎搜索
    * 短视频搜索：抖音搜索；短视频搜索
    * 网上商城搜索：天猫搜索；京东搜索
    * APP搜索：美团搜索

* 智能推荐
    * 短视频推荐：抖音推荐
    * 网上商城推荐：天猫推荐
    * APP推荐：美团推荐
    * 企业营销推荐：银行理财产品营销推荐

* 知识图谱
    * 通用知识图谱：搜索引擎
    * 领域图谱：问答系统

* 语义计算
    * 基础语义表示，预训练

* 多模态
    * 多模态对话问答
    * 多模态搜索
    * 多模态推荐

* 篇章理解
    * 篇章问答

* 数据挖掘
    * 基础数据挖掘，数据分析

* 机器翻译

### 互联网公司

* 基础nlp部门：对话系统；语义计算/语义理解；知识图谱；篇章理解；机器翻译
* ToC业务：搜索；推荐；文本挖掘；视频理解
* ToB业务：云平台；政企业务

### 行业公司

* 金融：营销推荐；客服机器人
* 政企：数据挖掘；质量审核；文本理解
* 教育：作业批改；智能翻译

---

## 知识点梳理

### 机器学习知识点

| 知识点       | 考点内容                                                   | 难易程度 | 备注 |
|--------------|------------------------------------------------------------|----------|------|
| 线性回归     | 1，能解决什么问题；2，过拟合，欠拟合如何解决               | 一般     |      |
| 逻辑回归     | 1，什么是Sigmoid函数；2，逻辑回归有什么优点以及应用        | 一般     |      |
| 决策树       | 1，ID3算法；2，分类决策树和回归决策树的区别                | 一般     |      |
| 随机森林     | 1，Bagging思想；2，随机森林有什么优缺点                    | 一般     |      |
| XGBoost      | 1，XGBoost和GBDT的区别                                     | 中等     |      |
| SVM          | 1，SVM原理；2，LR和SVM的联系与区别                         | 重点     |      |
| 贝叶斯网络   | 1，贝叶斯网络原理；2，贝叶斯网络；3生成模型与判别模型区别  | 重点     |      |
| 马尔科夫     | 马尔科夫网络，马尔科夫模型，马尔科夫过程，贝叶斯网络的区别 | 重点     |      |
| 主题模型     | 1，LDA模型；2，LDA的topic数如何确定                        | 一般     |      |
| 最大期望算法 | 1，极大似然函数的求解步骤；2，采用EM算法求解的模型有哪些   | 中等     |      |
| 聚类         | 1，什么是无监督算法；2，KNN与K-means有什么区别             | 中等     |      |
| KNN          | 1，什么是KNN；2，K值如何选                                 | 一般     |      |


### 深度学习

| 知识点   | 考点内容                                                        | 难易程度 | 备注 |
|----------|-----------------------------------------------------------------|----------|------|
| CNN      | 1，CNN如何卷积；2，什么是池化；3，CNN优缺点                     | 中等     |      |
| RNN      | 1，RNN结构；2，CNN与RNN区别；3，为什么RNN训练的时候Loss波动很大 | 中等     |      |
| GRU      | 1，门控单元都包含什么                                           | 一般     |      |
| LSTM     | 1，为什么会出现梯度消失，梯度爆炸，现象是什么，如何解决         | 重点     |      |
| 迁移学习 | 1，什么是迁移学习；有哪些方法                                   | 中等     |      |
| 强化学习 | 1，什么是强化学习，有哪些应用场景                               | 中等     |      |
| 集成学习 | 1，集成学习的好处                                               | 重点     |      |


### NLP基础任务

| 知识点   | 考点内容                                         | 难易程度 | 备注 |
|----------|--------------------------------------------------|----------|------|
| 语义表示 | 1，语义表示的演化过程                            | 重点     |      |
| 词性标注 | 1，词性标注的任务是什么，有哪些方法              | 一般     |      |
| 语义相似 | 1，语义相似模型如何构建；2，什么是负采样         | 重点     |      |
| 对话问答 | 1，对话问答的方法一般有哪些；2，对话中有哪些问题 | 重点     |      |
| 信息抽取 | 1，信息抽取任务是什么；2，如何做                 | 重点     |      |
| 图谱问答 | 1，图谱问答优势；2，如何构建图谱问答             | 重点     |      |
| 语义解析 | 1，语义解析任务是什么；2.常用方法有哪些          | 重点     |      |



### NLP备考重点

| 知识点                    | 考点内容                                                                                                                                                                 | 难易程度 | 备注 |
|---------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|------|
| 语言模型                  | 语言模型基础：什么是语言模型；语言模型发展史；语言模型结构；预训练语言模型 模型优化改进：不同语言模型优缺点；模型优化方法；模型改进趋势 模型训练问题：模型压缩；模型优化 | 重点     |      |
| N-gram                    | N-gram参数估计                                                                                                                                                           | 重点     |      |
| 语言模型评估              | 如何评估语言模型的好坏                                                                                                                                                   | 重点     |      |
| 数据稀疏                  | 数据稀疏问题如何解决（单词出现次数少）                                                                                                                                   | 重点     |      |
| 极大似然估计              | 极大似然估计 概念                                                                                                                                                        | 重点     |      |
| BP                        | 手推BP                                                                                                                                                                   | 重点     |      |
| 参数量估计                | 给出词表大小的模型参数量估计                                                                                                                                             | 重点     |      |
| 激活函数                  | 激活函数都有哪些，分别优缺点                                                                                                                                             | 重点     |      |
| word2vec                  | Wordvec有两种算法？算法区别是什么？                                                                                                                                      | 重点     |      |
| word2vec                  | word2vec为什么能学习到每个单词的向量表示                                                                                                                                 | 重点     |      |
| CBOW                      | CBOW模型有哪些问题                                                                                                                                                       | 重点     |      |
| CBOW                      | 如何解决CBOW梯度爆炸的问题                                                                                                                                               | 重点     |      |
| hierarchical softmax      | 什么是hierarchical softmax？                                                                                                                                             | 重点     |      |
| hierarchical softmax      | hierarchical softmax代替softmax后复杂度变成了多少                                                                                                                        | 重点     |      |
| fasttext                  | fasttext主要特点                                                                                                                                                         | 重点     |      |
| 负采样                    | 什么是负采样 负采样有什么作用 如何更好得选择负样本，即负采样？                                                                                                           | 重点     |      |
| Glove                     | Glove训练过程？                                                                                                                                                          | 重点     |      |
| Glove，word2vec           | Glove和word2vec各自有啥优缺点？                                                                                                                                          | 重点     |      |
| Glove                     | word2vec等方法构建词向量有什么问题？预训练语言模型的优势是什么？                                                                                                         | 重点     |      |
| ELMo                      | ELMo预训练语言模型网络结构？                                                                                                                                             | 重点     |      |
| GPT，ELMo                 | GPT与ELMo的区别？                                                                                                                                                        | 重点     |      |
| Self-attention            | 对于Self-attention的理解？与普通attention的区别                                                                                                                          | 重点     |      |
| multi-head attention      | multi-head attention 与self-attention的联系？                                                                                                                            | 重点     |      |
| Transformer               | transformer的网络结构                                                                                                                                                    | 重点     |      |
| Transformer               | Transformer的残差连接和归一化有何作用？                                                                                                                                  | 重点     |      |
| BERT                      | BERT和GPT的区别？                                                                                                                                                        | 重点     |      |
| BERT                      | BERT中的问题和改进思路                                                                                                                                                   | 重点     |      |
| NNLM，Word2vec            | 相对于NNLM，Word2vec的改进都有哪些                                                                                                                                       | 重点     |      |
| 哈夫曼树                  | 哈夫曼树的构建方法，在NLP中有啥应用                                                                                                                                      | 重点     |      |
| 分层softmax和负采样       | 分层softmax和负采样的原理和复杂度                                                                                                                                        | 重点     |      |
| 负采样                    | 负采样的具体实现方法                                                                                                                                                     | 重点     |      |
| 评估词向量                | 怎样评估词向量的质量                                                                                                                                                     | 重点     |      |
| 词向量相似度              | 选出当前query和100万个key词向量相似度的TopK，复杂度尽可能低（faiss）                                                                                                     | 重点     |      |
| 预训练模型                | 预训练模型word2vec-glove-ELMo-GPT-BERT-others的演进，每个模型分别解决什么问题                                                                                            | 重点     |      |
| 乱序语言模型，Transformer | 为什么Transformer可以实现乱序语言模型，怎么实现的                                                                                                                        | 重点     |      |
| 乱序语言模型              | 什么是乱序语言模型                                                                                                                                                       | 重点     |      |
| mask                      | attention矩阵的mask方式与各种预训练方案的关系                                                                                                                            | 重点     |      |
| 预训练bert                | 为什么直接利用预训练bert模型来做seq2seq任务                                                                                                                              | 重点     |      |
| 多头attention             | 为什么要用多头（多个空间学习多种pattern，降低注意力学习的风险）                                                                                                          | 重点     |      |
| Q和K的映射矩阵            | 为什么Q和K的映射矩阵不相同（关系对称，容易得到单位矩阵）                                                                                                                 | 重点     |      |
| 注意力权重                | 为什么注意力权重要除（防止梯度消失）                                                                                                                                     | 重点     |      |
| 乘性注意力                | 为什么用乘性注意力不用加性注意力（乘性计算更小？）                                                                                                                       | 重点     |      |
| FFN                       | 为什么用FFN模块（增加模型的非线性能力）                                                                                                                                  | 重点     |      |
| 位置编码                  | Transformer和BERT的位置编码有啥区别（三角函数式和可学习向量）                                                                                                            | 重点     |      |
| 残差                      | 残差结构及意义（防止梯度消失和网络退化）                                                                                                                                 | 重点     |      |
| Transformer               | 哪个block中更耗时，哪个更占显存（序列短的时候FFN耗时，长的时候MHA耗时；FFN更占显存）                                                                                     | 重点     |      |
| Transformer的layerNorm    | Transformer的layerNorm有哪些（post-norm 和 pre-norm）                                                                                                                    | 重点     |      |
| Transformer加速           | Transformer加速（NAR，知识蒸馏，剪枝，动态退出，稀疏注意力，线性注意力）                                                                                                 | 重点     |      |
| attention瓶颈             | attention瓶颈（low rank，talking-head）                                                                                                                                  | 重点     |      |




### 算法重点考点

一般：要求懂原理，应用
重点：手撸
* 最短路算法（一般）：Dijkstra / Floyd / SPFA
* 拓扑排序算法（一定会考）：Topological Sorting
* Morris算法（一般）：O(1)额外空间前序遍历
* 贪心法（重要）：greedy
* Manacher算法（一般）：求最长回文子串
* KMP算法（一般）：strstr / indexOf
* 最小生成树算法（重要）：Minimun Spanning Tree
* 二分法（一定会考）：Binary Search
* 分治法（重要）：Divede & Conquer
* 网络流算法（一般）：Network Flow
* 希尔排序（一般）：Shell Sort
* 动态规划（一定会考）：Dynamic Programming
* 线段树（一般）：Segment Tree
* 平衡排序二叉树（一般）：Red-black Tree
* 字典树（重要）：Trie
* 并查集（重要）：Union Find
* 跳跃表（一般）：Skip List
* 哈希表（一定会考）：Hash Table
* 堆（重要）：Heap
* KD树（一般）：KD-Tree
* B树/B+树（一般）：B-Tree / B+ Tree
* 二叉查找树（一定会考）：Binary Search Tree




----



## 答案

### 机器学习知识点
>* 线性回归
    1，能解决什么问题：对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。
    2，过拟合，欠拟合如何解决：使用正则化项，也就是给loss function加上一个参数项，正则化项有L1正则化（Lasso回归）、L2正则化（岭回归）、ElasticNet。以控制参数1篇幅，限制参数搜索空间，解决过拟合和欠拟合的问题。
    $$Loss_{MSE} = \frac{1}{2m} \Sigma^{i=1}_{m} (y' - y)^2$$  
    $$Loss_{MSE+L2} = \frac{1}{2m} \Sigma^{i=1}_{m} (y' - y)^2 + \lambda \Sigma^{i=1}_{w} w_{i}^{2} $$
    只要数据线性相关，用LinearRegression拟合的不是很好，需要正则化，可以考虑使用岭回归(L2), 如何输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。
    $$Loss_{MSE+L1} = \frac{1}{2m} \Sigma^{i=1}_{m} (y' - y)^2 + \lambda \Sigma^{i=1}_{w} |w_i| $$
    L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。  
    ElasticNet综合了L1正则化项和L2正则化项，

>* 逻辑回归  
    1，什么是Sigmoid函数：  
    $$ Sigmoid(t) = \frac{1}{1+e^{-t}}$$  
    函数中t无论取什么值，其结果都在[0,-1]的区间内, 把aX+b带入t中就得到了我们的逻辑回归的一般模型方程:
    $$H(a,b) = \frac{1}{1+e^{aX+b}} $$  
    逻辑回归的损失函数是 log loss，也就是对数似然函数，函数公式如下：  
    $$
    \operatorname{Cost}\left(h_\theta(x), y\right)=\left\{\begin{aligned}
        -\log \left(h_\theta(x)\right) & \text { if } y=1 \\
    -\log \left(1-h_\theta(x)\right) & \text { if } y=0
    \end{aligned}\right.
    $$
    公式中的 y=1 表示的是真实值为1时用第一个公式，真实 y=0 用第二个公式计算损失。为什么要加上log函数呢？可以试想一下，当真实样本为1是，但h=0概率，那么log0=∞，这就对模型最大的惩罚力度；当h=1时，那么log1=0，相当于没有惩罚，也就是没有损失，达到最优结果。所以数学家就想出了用log函数来表示损失函数
    2，逻辑回归有什么优点以及应用：
        * 优点：  
            LR能以概率的形式输出结果，而非只是0,1判定。  
            LR的可解释性强，可控度高(你要给老板讲的嘛…)。  
            训练快，feature engineering之后效果赞。  
            因为结果是概率，可以做ranking model。  
        * 应用
            CTR预估/推荐系统的learning to rank/各种分类场景。  
            某搜索引擎厂的广告CTR预估基线版是LR。  
            某电商搜索排序/广告CTR预估基线版是LR。  
            某电商的购物搭配推荐用了大量LR。  
            某现在一天广告赚1000w+的新闻app排序基线是LR  

>* 决策树
    1，ID3算法：  
    信息熵: 
    $$\operatorname{Ent}(D)=-\sum_{k=1}^{|y|} p_k \log _2 p_k$$  
    在根节点处计算信息熵，然后根据属性依次划分并计算其节点的信息熵，用根节点信息熵--属性节点的信息熵=信息增益，根据信息增益进行降序排列，排在前面的就是第一个划分属性，其后依次类推，这就得到了决策树的形状，也就是怎么“长”了。
    2，分类决策树和回归决策树的区别：CART回归树实质上就是在该特征维度对样本空间进行划分，而这种空间划分的优化是一种NP难问题，因此，在决策树模型中是使用启发式方法解决。

>* 随机森林	
    1，Bagging思想：Bagging是bootstrap aggregating。思想就是从总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。Random Forest(随机森林)是一种基于树模型的Bagging的优化版本，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的特点。而同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。
    1.1，Boosting思想
    Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。
    2，随机森林有什么优缺点：
    优点：
    * 在当前的很多数据集上，相对其他算法有着很大的优势，表现良好。
    * 它能够处理很高维度（feature很多）的数据，并且不用做特征选择(因为特征子集是随机选择的)。
    * 在训练完后，它能够给出哪些feature比较重要。
    * 训练速度快，容易做成并行化方法(训练时树与树之间是相互独立的)。
    * 在训练过程中，能够检测到feature间的互相影响。
    * 对于不平衡的数据集来说，它可以平衡误差。
    * 如果有很大一部分的特征遗失，仍可以维持准确度。
    缺点：
    * 随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合。
    * 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。

>* XGBoost
    1，XGBoost和GBDT的区别：
    * GBDT是机器学习算法，XGBoost是该算法的工程实现。
    * 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。
    * GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
    * 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。
    * 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样。
    * 传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。

>* SVM
    1，SVM原理：
    2，LR和SVM的联系与区别：  
    相同点  
    * 都是线性分类器。本质上都是求一个最佳分类超平面。
    * 都是监督学习算法。
    * 都是判别模型。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。  
    不同点  
    * LR是参数模型，svm是非参数模型，linear和rbf则是针对数据线性可分和不可分的区别；
    * 从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
    * SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
    * 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
    * logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

>* 贝叶斯网络
    1，贝叶斯网络原理：贝叶斯网络(Bayesian network)，又称信念网络(Belief Network)，或有向无环图模型(directed acyclic graphical model)，是一种概率图模型，于1985年由Judea Pearl首先提出。它是一种模拟人类推理过程中因果关系的不确定性处理模型，其网络拓朴结构是一个有向无环图(DAG)。
    2，贝叶斯网络：
    3，生成模型与判别模型区别：
    * 判别模型(discriminative model)通过求解条件概率分布P(y|x)或者直接计算y的值来预测y。
    * 线性回归（Linear Regression）,逻辑回归（Logistic Regression）,支持向量机（SVM）, 传统神经网络（Traditional Neural Networks）,线性判别分析（Linear Discriminative Analysis），条件随机场（Conditional Random Field）
    * 生成模型（generative model）通过对观测值和标注数据计算联合概率分布P(x,y)来达到判定估算y的目的。
    * 朴素贝叶斯（Naive Bayes）, 隐马尔科夫模型（HMM）,贝叶斯网络（Bayesian Networks）和隐含狄利克雷分布（Latent Dirichlet Allocation）、混合高斯模型

>* 马尔科夫  
    * 将随机变量作为结点，若两个随机变量相关或者不独立，则将二者连接一条边；若给定若干随机变量，则形成一个有向图，即构成一个网络。
    * 如果该网络是有向无环图，则这个网络称为贝叶斯网络。
    * 如果这个图退化成线性链的方式，则得到马尔可夫模型；因为每个结点都是随机变量，将其看成各个时刻(或空间)的相关变化，以随机过程的视角，则可以看成是马尔可夫过程。
    * 若上述网络是无向的，则是无向图模型，又称马尔可夫随机场或者马尔可夫网络。
    * 如果在给定某些条件的前提下，研究这个马尔可夫随机场，则得到条件随机场。
    * 如果使用条件随机场解决标注问题，并且进一步将条件随机场中的网络拓扑变成线性的，则得到线性链条件随机场。

>* 主题模型
    1，LDA模型：
    2，LDA的topic数如何确定：
    * 基于经验 主观判断、不断调试、操作性强、最为常用。
    * 基于困惑度（主要是比较两个模型之间的好坏）。
    * 使用Log-边际似然函数的方法，这种方法也挺常用的。
    * 非参数方法：Teh提出的基于狄利克雷过程的HDP法。
    * 基于主题之间的相似度：计算主题向量之间的余弦距离，KL距离等

其余参照https://github.com/NLP-LOVE/ML-NLP




### 语言模型相关考点

> 语言模型基础：什么是语言模型；语言模型发展史；语言模型结构；预训练语言模型  
> 模型优化改进：不同语言模型优缺点；模型优化方法；模型改进趋势  
> 模型训练问题：模型压缩；模型优化  

* 语言模型基础介绍
  * 什么是语言模型：语言建模的目的就是构建该自然语言处理中词序列的分布，然后用于评估某个词序列的概率。如果给定的词序列符合语用习惯则给出高概率，否则给出低概率
  * 语言模型基础是马尔科夫假设
  * 语言模型计算采用链式法则，单个词序列的概率被分解为序列中各个词的条件概率的乘积，而每个词的条件概率为给定其上下文时改词出现的概率。


$$ p(S) = \prod^{T}_{i=1} p(w_i | w_1,...,w_{i-1}) $$


>* 考点：N-gram参数估计  
    Unigram: $2 \times 10^5$  
    Bigram: $(2\times10^5)^2=4\times10^{10}$  
    Trigram: $(2\times10^5)^3=8\times10^{15}$  
    4-gram: $(2\times10^5)^4=16\times10^{20}$  

>* 考点：如何评估语言模型的好坏
1，困惑度（preplexity）：困惑度定义为测试集的概率的倒数，并用单词数做归一化

>* 考点：数据稀疏问题如何解决（单词出现次数少）
大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题  
可用加一平滑法（拉普拉斯定律）解决
$$p_{Add-1}(w_t) = \frac{count(w_t)+1}{T+|V|}$$

>* 考点：极大似然估计 概念

>* 考点：手推BP

>* 考点：神经网络语音模型相较于ngram优点  
2003年bangio提出神经网络语言模型NNLM，提出了词向量的概念，代替了ngram所使用的离散变量而采用连续变量来进行单词的分布式表示，解决了维度灾难和数据稀疏性的问题，同时通过词向量可获得词之间的相似性。

>* 考点：给出词表大小的模型参数量估计
李沐视频

>* 考点：激活函数都有哪些，分别优缺点

>* 考点：Wordvec有两种算法？算法区别是什么？
Skip-garam和CBOW，他们最大的区别是前者是通过中心词预测周边词，后者是用周边词来预测中心词

>* 考点：word2vec为什么能学习到每个单词的向量表示

>* 考点：CBOW模型有哪些问题　　
CBOW模型是用上下文 $X$ 来预测中间词 $Y$ ,那么其输出层（输出层1 * $V$ 的向量）有 $V$ 个神经元，我们对这 $V$ 个神经元一开始是等同对待的，但是如果 $V$ 的数值非常大会导致效率过低，计算量过大。

>* 考点：如何解决CBOW梯度爆炸的问题  
使用哈夫曼树来代替从隐藏层输出softmax层的映射，（哈夫曼树的性质，越靠近根节点的地方，词频（权重）越高，我们可以更快的使用这个词，相反的越靠近叶节点的地方，词频率越低，这两就提高了训练效果）。在哈夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着哈夫曼树一步步完成的，因此这种softmax取名为hierarchical softmax，即分层softmax。

>* 考点：什么是hierarchical softmax？  
根据标签（label）和频率建立哈夫曼树；（label出现的频率越高，huffman树的路径越短，huffman树中的每一个叶子结点代表一个label）

>* 考点：hierarchical softmax代替softmax后复杂度变成了多少
复杂度从 $O(N^2)$ 变成了 $O(\log N)$

>* 考点：fasttext主要特点  
fasttext最早其实是一个文本分类算法，后续加了一些改进来训练词向量，主要有：
    * fastext在输入时对每个词加入了n-gram特征，在输出时使用分层softmax加速训练。
    * fasttext将整篇文章的词向量求平均作为输入得到文档向量，用文本分类做有监督训练，对输出进行softmax回归，词向量为副产品。
    * fasttext也可以无监督训练词向量，与CBOW非常相似。

>* 考点：什么是负采样
>* 考点：负采样有什么作用
>* 考点：如何更好得选择负样本，即负采样？
    如skipgram 1，一个办法是对中间的这些词进行采样，即候选的目标词，根据其在语料中的经验频率（empirical frequency）进行采样，就是通过词出现的频率进行采样，但是这样会导致在like，the，of，and诸如此类的词上有很好高的频率。
    2，另一个极端就是用1除以词汇表总词数，即 $\frac{1}{|V|}$, 均匀且随机地抽取负样本，这对于英文单词的分布是非常没有代表性的。
    3，论文的作者Mikolov等人根据经验，他们发现这个经验值的效果最好，它位于这两个极端的采样方法之间，既不用经验频率，也就是实际观察到的英文文本的分布，也不用均匀分布，他们采用以下方式:
    $$p(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\Sigma^{10,000}_{j=1}f(w_j)^{\frac{3}{4}}} $$
    上式的 $f(w_i)$ 是观测到的在语料库中的某个英文词的词频，通过 $\frac{3}{4}$ 次方的计算，使其处于完全独立的分布和训练集的观测分布两个极端之间。Andrew并不确定这是否有理论证明，但是很多研究者现在使用这个方法似乎效果也不错

>* 考点：Glove训练过程？
    1，构建共现矩阵：Glove指定特定大小的上下文窗口，通过滑动该窗口统计共现矩阵 $X(|V| \times |V|)$,$X_{ij}$ 表示中心词i与上下文j的共现次数。同时还定义了衰减函数，令距离为d的两个词在计数时乘以 $\frac{1}{d}$  
    2，确定近似目标：作者发现可以用概率之比来建模共现关系，定义条件概率 $P$ :
    $$P_{ij} = P(j|i) = \frac{X_{ij}}{X_i}$$  
    3，表示词 $j$ 出现在 $i$ 上下文的概率。而用词 $k$ 出现在 $i$ 的上下文与它出现在 $j$ 上下文的概率之比：
    $$ratio_{ijk} = \frac{P_{ik}}{P_{jk}}$$  
    来表示 $i$, $j$, $k$ 三个词的共现关系。当 $i$, $k$ 和 $j$, $k$相关程度近似时，该比率趋近于1；$i$，$k$ 相关度大于$j$，$k$相关度时该比率值较大，反之较小。Glove的目标就是使学习到的词向量满足这样的规律，既有自身上下文信息，又能和其他词联系起来。目标函数：
    $$F(w_i, w_j, w_k) = \frac{P_{ik}}{P_{jk}}$$  

>* 考点：Glove和word2vec各自有啥优缺点？
    1，Word2vec面向局部特征，基于滑动窗口，而Glove综合了全局语料。  
    2，word2vec可以增量学习，而Glove是由固定语料计算的共现矩阵。

>* 考点：word2vec等方法构建词向量有什么问题？预训练语言模型的优势是什么？
    1， word embedding等都是独立于上下文的，也就是说无论下有任务是什么，输入的word embedding始终是固定的，这就无法解决一词多义，以及在不同语境下有不同表现的需求  
    2，ELMo，GPT以及BERT等都是一次多义以及能够适配下游任务，通过预训练和finetune两个阶段来构造context-dependence的词的表示。

>* 考点：ELMo预训练语言模型网络结构？
    1，ELMo使用双向语言模型来进行预训练，用两个分开的双层LSTM作为encoder，第一层学到的是句法信息，第二层学到的是语义信息。在面对具体下游任务时，首先固定biLM的参数的到一个词的表示，再与上下文无关的词表示（Word2vec，或者charCNN获得的表示）拼接作为模型输入，在反向传播finetune所有参数。

>* 考点：GPT与ELMo的区别？
    ELMo使用LSTM作为编码器，而GPT开始用的是编码能力更强的transformer

>* 考点：对于Self-attention的理解？与普通attention的区别  
    如何学习一个更好的序列表示RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；attention的思路最为粗暴，它一步到位获得了全局信息：
    $$y_t = f(x_t, A, B) $$
其中 $A$，$B$ 是另一个序列（矩阵）。如果 $A = B = X$, 那么就称为Self-attention, 它的意思是直接将$x_t$与原来的每个词进行比较，最后算出$y_t$

>* 考点：multi-head attention 与self-attention的联系？
    这个是google提出的新概念，是attention机制的完善，只是把 $Q$，$K$，$V$通过矩阵映射一下，然后再做Attention，把这个过程重复做 $h$ 次 结果拼接起来就行了

>* 考点：transformer的网络结构
    1，架构上也是一种encode-decode的架构
    2，每个block单元都包含自注意力计算模块，前馈网络，残差模块（Add&Norm，恒等映射的残差连接）

>* 考点：Transformer的残差连接和归一化有何作用？
    1，深层网络会存在梯度消失/爆炸的情况，可以通过Normalization等方式解决，是的模型能够收敛。
    2，在模型能够收敛的情况下，网络越深，模型的准确率越低，同时，模型的准确率先达到饱和然后迅速下降。这个情况我们称之为网络退化。借助ResNet，我们能够有效训练出更深的网络模型（可以超过1000层），使得网络表现不亚于浅层网络。

>* 考点：BERT和GPT的区别？
    1，训练数据不同，GPT使用BooksCorpus（800M words）；BERT是BooksCorpus（800M words）加Wikipedia（2500Mwords）
    2，GPT在训练时没有【CLS】和【SEP】，在下游任务时才有
    3，GPT在finetuning时加入LM的Loss，而Bert是完全使用任务特定目标函数。
    4，GPT的lr在两个1阶段保持一致，Bert认为特定任务的lr效果更好
    5，Bert用mask来实现了双向语言模型，非常巧妙
    6，预训练除了语言模型，还加入了next Sentence prediction，试图学习更高层面的语言相关性。

>* 考点：BERT中的问题和改进思路  
    在实际使用中Q，K，V一般具有相同的维度特征（即hidden—size）比如BERT Base里面是 $d$ = 768, $h$ = 12, 映射矩阵为
    $$W \in R^{d \times (d/h)} $$  
    也就是说每个Attention head里面是将$d$映射到$d/h$,然后再进行Attention计算，输出也是$d/h$维，最后把$h$个$d/h$维的结果拼接起来，得到一个$d$维的输出。这里的$d/h$通常称为head_size  
    在Attention中  
    $$P = softmax (\frac{QK^{T}}{\sqrt{d_k}})$$  
    可以将P看成一个二元联合分布eg.$N^2$，但是我们的参数量只有$n\times (d/h)$ 这就导致了低秩瓶颈（Low-Rank Bottleneck），解决方法是增大模型的key_size也就是Q，K的尺寸。

>* 考点：相对于NNLM，Word2vec的改进都有哪些
>* 考点：哈夫曼树的构建方法，在NLP中有啥应用
>* 考点：分层softmax和负采样的原理和复杂度
>* 考点：负采样的具体实现方法
>* 考点：怎样评估词向量的质量
>* 考点：选出当前query和100万个key词向量相似度的TopK，复杂度尽可能低（faiss）
>* 考点：预训练模型word2vec-glove-ELMo-GPT-BERT-others的演进，每个模型分别解决什么问题

Transformer
>* 考点：为什么Transformer可以实现乱序语言模型，怎么实现的
>* 考点：什么是乱序语言模型
>* 考点：attention矩阵的mask方式与各种预训练方案的关系
>* 考点：直接利用预训练bert模型来做seq2seq任务
>* 考点：为什么要用多头（多个空间学习多种pattern，降低注意力学习的风险）
>* 考点：为什么Q和K的映射矩阵不相同（关系对称，容易得到单位矩阵）
>* 考点：为什么注意力权重要除（防止梯度消失）
>* 考点：为什么用乘性注意力不用加性注意力（乘性计算更小？）
>* 考点：为什么FFN模块（增加模型的非线性能力）
>* 考点：Transformer和BERT的位置编码有啥区别（三角函数式和可学习向量）
>* 考点：残差结构及意义（防止梯度消失和网络退化）
>* 考点：哪个block中更耗时，哪个更占显存（序列短的时候FFN耗时，长的时候MHA耗时；FFN更占显存）
>* 考点：Transformer的layerNorm有哪些（post-norm 和 pre-norm）
>* 考点：Transformer加速（NAR，知识蒸馏，剪枝，动态退出，稀疏注意力，线性注意力）
>* 考点：attention瓶颈（low rank，talking-head）





