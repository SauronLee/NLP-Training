## 就业方向

### 岗位方向

* 智能对话
    * 智能客服：京东，淘宝在线客服；快递售后机器人；政企只能外呼；海底捞智能预约机器人；网上商城售后机器人
    * 智能音箱：小度音响；天猫精灵；小爱同学
    * 智能硬件：车载导航；siri

* 智能搜索
    * 搜索引擎搜索：百度搜索；知乎搜索
    * 短视频搜索：抖音搜索；短视频搜索
    * 网上商城搜索：天猫搜索；京东搜索
    * APP搜索：美团搜索

* 智能推荐
    * 短视频推荐：抖音推荐
    * 网上商城推荐：天猫推荐
    * APP推荐：美团推荐
    * 企业营销推荐：银行理财产品营销推荐

* 知识图谱
    * 通用知识图谱：搜索引擎
    * 领域图谱：问答系统

* 语义计算
    * 基础语义表示，预训练

* 多模态
    * 多模态对话问答
    * 多模态搜索
    * 多模态推荐

* 篇章理解
    * 篇章问答

* 数据挖掘
    * 基础数据挖掘，数据分析

* 机器翻译

### 互联网公司

* 基础nlp部门：对话系统；语义计算/语义理解；知识图谱；篇章理解；机器翻译
* ToC业务：搜索；推荐；文本挖掘；视频理解
* ToB业务：云平台；政企业务

### 行业公司

* 金融：营销推荐；客服机器人
* 政企：数据挖掘；质量审核；文本理解
* 教育：作业批改；智能翻译

---

## 知识点梳理

### 机器学习知识点

| 知识点       | 考点内容                                                   | 难易程度 | 备注 |
|--------------|------------------------------------------------------------|----------|------|
| 线性回归     | 1，能解决什么问题；2，过拟合，欠拟合如何解决               | 一般     |      |
| 逻辑回归     | 1，什么是Sigmoid函数；2，逻辑回归有什么优点以及应用        | 一般     |      |
| 决策树       | 1，ID3算法；2，分类决策树和回归决策树的区别                | 一般     |      |
| 随机森林     | 1，Bagging思想；2，随机森林有什么优缺点                    | 一般     |      |
| XGBoost      | 1，XGBoost和GBDT的区别                                     | 中等     |      |
| SVM          | 1，SVM原理；2，LR和SVM的联系与区别                         | 重点     |      |
| 贝叶斯网络   | 1，贝叶斯网络原理；2，贝叶斯网络；3生成模型与判别模型区别  | 重点     |      |
| 马尔科夫     | 马尔科夫网络，马尔科夫模型，马尔科夫过程，贝叶斯网络的区别 | 重点     |      |
| 主题模型     | 1，LDA模型；2，LDA的topic数如何确定                        | 一般     |      |
| 最大期望算法 | 1，极大似然函数的求解步骤；2，采用EM算法求解的模型有哪些   | 中等     |      |
| 聚类         | 1，什么是无监督算法；2，KNN与K-means有什么区别             | 中等     |      |
| KNN          | 1，什么是KNN；2，K值如何选                                 | 一般     |      |


### 深度学习

| 知识点   | 考点内容                                                        | 难易程度 | 备注 |
|----------|-----------------------------------------------------------------|----------|------|
| CNN      | 1，CNN如何卷积；2，什么是池化；3，CNN优缺点                     | 中等     |      |
| RNN      | 1，RNN结构；2，CNN与RNN区别；3，为什么RNN训练的时候Loss波动很大 | 中等     |      |
| GRU      | 1，门控单元都包含什么                                           | 一般     |      |
| LSTM     | 1，为什么会出现梯度消失，梯度爆炸，现象是什么，如何解决         | 重点     |      |
| 迁移学习 | 1，什么是迁移学习；有哪些方法                                   | 中等     |      |
| 强化学习 | 1，什么是强化学习，有哪些应用场景                               | 中等     |      |
| 集成学习 | 1，集成学习的好处                                               | 重点     |      |


### NLP基础任务

| 知识点   | 考点内容                                         | 难易程度 | 备注 |
|----------|--------------------------------------------------|----------|------|
| 语义表示 | 1，语义表示的演化过程                            | 重点     |      |
| 词性标注 | 1，词性标注的任务是什么，有哪些方法              | 一般     |      |
| 语义相似 | 1，语义相似模型如何构建；2，什么是负采样         | 重点     |      |
| 对话问答 | 1，对话问答的方法一般有哪些；2，对话中有哪些问题 | 重点     |      |
| 信息抽取 | 1，信息抽取任务是什么；2，如何做                 | 重点     |      |
| 图谱问答 | 1，图谱问答优势；2，如何构建图谱问答             | 重点     |      |
| 语义解析 | 1，语义解析任务是什么；2.常用方法有哪些          | 重点     |      |



### NLP备考重点

| 知识点                    | 考点内容                                                                                                                                                                 | 难易程度 | 备注 |
|---------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|------|
| 语言模型                  | 语言模型基础：什么是语言模型；语言模型发展史；语言模型结构；预训练语言模型 模型优化改进：不同语言模型优缺点；模型优化方法；模型改进趋势 模型训练问题：模型压缩；模型优化 | 重点     |      |
| N-gram                    | N-gram参数估计                                                                                                                                                           | 重点     |      |
| 语言模型评估              | 如何评估语言模型的好坏                                                                                                                                                   | 重点     |      |
| 数据稀疏                  | 数据稀疏问题如何解决（单词出现次数少）                                                                                                                                   | 重点     |      |
| 极大似然估计              | 极大似然估计 概念                                                                                                                                                        | 重点     |      |
| BP                        | 手推BP                                                                                                                                                                   | 重点     |      |
| 参数量估计                | 给出词表大小的模型参数量估计                                                                                                                                             | 重点     |      |
| 激活函数                  | 激活函数都有哪些，分别优缺点                                                                                                                                             | 重点     |      |
| word2vec                  | Wordvec有两种算法？算法区别是什么？                                                                                                                                      | 重点     |      |
| word2vec                  | word2vec为什么能学习到每个单词的向量表示                                                                                                                                 | 重点     |      |
| CBOW                      | CBOW模型有哪些问题                                                                                                                                                       | 重点     |      |
| CBOW                      | 如何解决CBOW梯度爆炸的问题                                                                                                                                               | 重点     |      |
| hierarchical softmax      | 什么是hierarchical softmax？                                                                                                                                             | 重点     |      |
| hierarchical softmax      | hierarchical softmax代替softmax后复杂度变成了多少                                                                                                                        | 重点     |      |
| fasttext                  | fasttext主要特点                                                                                                                                                         | 重点     |      |
| 负采样                    | 什么是负采样 负采样有什么作用 如何更好得选择负样本，即负采样？                                                                                                           | 重点     |      |
| Glove                     | Glove训练过程？                                                                                                                                                          | 重点     |      |
| Glove，word2vec           | Glove和word2vec各自有啥优缺点？                                                                                                                                          | 重点     |      |
| Glove                     | word2vec等方法构建词向量有什么问题？预训练语言模型的优势是什么？                                                                                                         | 重点     |      |
| ELMo                      | ELMo预训练语言模型网络结构？                                                                                                                                             | 重点     |      |
| GPT，ELMo                 | GPT与ELMo的区别？                                                                                                                                                        | 重点     |      |
| Self-attention            | 对于Self-attention的理解？与普通attention的区别                                                                                                                          | 重点     |      |
| multi-head attention      | multi-head attention 与self-attention的联系？                                                                                                                            | 重点     |      |
| Transformer               | transformer的网络结构                                                                                                                                                    | 重点     |      |
| Transformer               | Transformer的残差连接和归一化有何作用？                                                                                                                                  | 重点     |      |
| BERT                      | BERT和GPT的区别？                                                                                                                                                        | 重点     |      |
| BERT                      | BERT中的问题和改进思路                                                                                                                                                   | 重点     |      |
| NNLM，Word2vec            | 相对于NNLM，Word2vec的改进都有哪些                                                                                                                                       | 重点     |      |
| 哈夫曼树                  | 哈夫曼树的构建方法，在NLP中有啥应用                                                                                                                                      | 重点     |      |
| 分层softmax和负采样       | 分层softmax和负采样的原理和复杂度                                                                                                                                        | 重点     |      |
| 负采样                    | 负采样的具体实现方法                                                                                                                                                     | 重点     |      |
| 评估词向量                | 怎样评估词向量的质量                                                                                                                                                     | 重点     |      |
| 词向量相似度              | 选出当前query和100万个key词向量相似度的TopK，复杂度尽可能低（faiss）                                                                                                     | 重点     |      |
| 预训练模型                | 预训练模型word2vec-glove-ELMo-GPT-BERT-others的演进，每个模型分别解决什么问题                                                                                            | 重点     |      |
| 乱序语言模型，Transformer | 为什么Transformer可以实现乱序语言模型，怎么实现的                                                                                                                        | 重点     |      |
| 乱序语言模型              | 什么是乱序语言模型                                                                                                                                                       | 重点     |      |
| mask                      | attention矩阵的mask方式与各种预训练方案的关系                                                                                                                            | 重点     |      |
| 预训练bert                | 为什么直接利用预训练bert模型来做seq2seq任务                                                                                                                              | 重点     |      |
| 多头attention             | 为什么要用多头（多个空间学习多种pattern，降低注意力学习的风险）                                                                                                          | 重点     |      |
| Q和K的映射矩阵            | 为什么Q和K的映射矩阵不相同（关系对称，容易得到单位矩阵）                                                                                                                 | 重点     |      |
| 注意力权重                | 为什么注意力权重要除（防止梯度消失）                                                                                                                                     | 重点     |      |
| 乘性注意力                | 为什么用乘性注意力不用加性注意力（乘性计算更小？）                                                                                                                       | 重点     |      |
| FFN                       | 为什么用FFN模块（增加模型的非线性能力）                                                                                                                                  | 重点     |      |
| 位置编码                  | Transformer和BERT的位置编码有啥区别（三角函数式和可学习向量）                                                                                                            | 重点     |      |
| 残差                      | 残差结构及意义（防止梯度消失和网络退化）                                                                                                                                 | 重点     |      |
| Transformer               | 哪个block中更耗时，哪个更占显存（序列短的时候FFN耗时，长的时候MHA耗时；FFN更占显存）                                                                                     | 重点     |      |
| Transformer的layerNorm    | Transformer的layerNorm有哪些（post-norm 和 pre-norm）                                                                                                                    | 重点     |      |
| Transformer加速           | Transformer加速（NAR，知识蒸馏，剪枝，动态退出，稀疏注意力，线性注意力）                                                                                                 | 重点     |      |
| attention瓶颈             | attention瓶颈（low rank，talking-head）                                                                                                                                  | 重点     |      |







----



## 答案



### 语言模型相关考点

> 语言模型基础：什么是语言模型；语言模型发展史；语言模型结构；预训练语言模型  
> 模型优化改进：不同语言模型优缺点；模型优化方法；模型改进趋势  
> 模型训练问题：模型压缩；模型优化  

* 语言模型基础介绍
  * 什么是语言模型：语言建模的目的就是构建该自然语言处理中词序列的分布，然后用于评估某个词序列的概率。如果给定的词序列符合语用习惯则给出高概率，否则给出低概率
  * 语言模型基础是马尔科夫假设
  * 语言模型计算采用链式法则，单个词序列的概率被分解为序列中各个词的条件概率的乘积，而每个词的条件概率为给定其上下文时改词出现的概率。


$$ p(S) = \prod^{T}_{i=1} p(w_i | w_1,...,w_{i-1}) $$


>* 考点：N-gram参数估计  
    Unigram: $2 \times 10^5$  
    Bigram: $(2\times10^5)^2=4\times10^{10}$  
    Trigram: $(2\times10^5)^3=8\times10^{15}$  
    4-gram: $(2\times10^5)^4=16\times10^{20}$  

>* 考点：如何评估语言模型的好坏
1，困惑度（preplexity）：困惑度定义为测试集的概率的倒数，并用单词数做归一化

>* 考点：数据稀疏问题如何解决（单词出现次数少）
大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题  
可用加一平滑法（拉普拉斯定律）解决
$$p_{Add-1}(w_t) = \frac{count(w_t)+1}{T+|V|}$$

>* 考点：极大似然估计 概念

>* 考点：手推BP

>* 考点：神经网络语音模型相较于ngram优点  
2003年bangio提出神经网络语言模型NNLM，提出了词向量的概念，代替了ngram所使用的离散变量而采用连续变量来进行单词的分布式表示，解决了维度灾难和数据稀疏性的问题，同时通过词向量可获得词之间的相似性。

>* 考点：给出词表大小的模型参数量估计
李沐视频

>* 考点：激活函数都有哪些，分别优缺点

>* 考点：Wordvec有两种算法？算法区别是什么？
Skip-garam和CBOW，他们最大的区别是前者是通过中心词预测周边词，后者是用周边词来预测中心词

>* 考点：word2vec为什么能学习到每个单词的向量表示

>* 考点：CBOW模型有哪些问题　　
CBOW模型是用上下文 $X$ 来预测中间词 $Y$ ,那么其输出层（输出层1 * $V$ 的向量）有 $V$ 个神经元，我们对这 $V$ 个神经元一开始是等同对待的，但是如果 $V$ 的数值非常大会导致效率过低，计算量过大。

>* 考点：如何解决CBOW梯度爆炸的问题  
使用哈夫曼树来代替从隐藏层输出softmax层的映射，（哈夫曼树的性质，越靠近根节点的地方，词频（权重）越高，我们可以更快的使用这个词，相反的越靠近叶节点的地方，词频率越低，这两就提高了训练效果）。在哈夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着哈夫曼树一步步完成的，因此这种softmax取名为hierarchical softmax，即分层softmax。

>* 考点：什么是hierarchical softmax？  
根据标签（label）和频率建立哈夫曼树；（label出现的频率越高，huffman树的路径越短，huffman树中的每一个叶子结点代表一个label）

>* 考点：hierarchical softmax代替softmax后复杂度变成了多少
复杂度从 $O(N^2)$ 变成了 $O(\log N)$

>* 考点：fasttext主要特点  
fasttext最早其实是一个文本分类算法，后续加了一些改进来训练词向量，主要有：
    * fastext在输入时对每个词加入了n-gram特征，在输出时使用分层softmax加速训练。
    * fasttext将整篇文章的词向量求平均作为输入得到文档向量，用文本分类做有监督训练，对输出进行softmax回归，词向量为副产品。
    * fasttext也可以无监督训练词向量，与CBOW非常相似。

>* 考点：什么是负采样
>* 考点：负采样有什么作用
>* 考点：如何更好得选择负样本，即负采样？
    如skipgram 1，一个办法是对中间的这些词进行采样，即候选的目标词，根据其在语料中的经验频率（empirical frequency）进行采样，就是通过词出现的频率进行采样，但是这样会导致在like，the，of，and诸如此类的词上有很好高的频率。
    2，另一个极端就是用1除以词汇表总词数，即 $\frac{1}{|V|}$, 均匀且随机地抽取负样本，这对于英文单词的分布是非常没有代表性的。
    3，论文的作者Mikolov等人根据经验，他们发现这个经验值的效果最好，它位于这两个极端的采样方法之间，既不用经验频率，也就是实际观察到的英文文本的分布，也不用均匀分布，他们采用以下方式:
    $$p(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\Sigma^{10,000}_{j=1}f(w_j)^{\frac{3}{4}}} $$
    上式的 $f(w_i)$ 是观测到的在语料库中的某个英文词的词频，通过 $\frac{3}{4}$ 次方的计算，使其处于完全独立的分布和训练集的观测分布两个极端之间。Andrew并不确定这是否有理论证明，但是很多研究者现在使用这个方法似乎效果也不错

>* 考点：Glove训练过程？
    1，构建共现矩阵：Glove指定特定大小的上下文窗口，通过滑动该窗口统计共现矩阵 $X(|V| \times |V|)$,$X_{ij}$ 表示中心词i与上下文j的共现次数。同时还定义了衰减函数，令距离为d的两个词在计数时乘以 $\frac{1}{d}$  
    2，确定近似目标：作者发现可以用概率之比来建模共现关系，定义条件概率 $P$ :
    $$P_{ij} = P(j|i) = \frac{X_{ij}}{X_i}$$  
    3，表示词 $j$ 出现在 $i$ 上下文的概率。而用词 $k$ 出现在 $i$ 的上下文与它出现在 $j$ 上下文的概率之比：
    $$ratio_{ijk} = \frac{P_{ik}}{P_{jk}}$$  
    来表示 $i$, $j$, $k$ 三个词的共现关系。当 $i$, $k$ 和 $j$, $k$相关程度近似时，该比率趋近于1；$i$，$k$ 相关度大于$j$，$k$相关度时该比率值较大，反之较小。Glove的目标就是使学习到的词向量满足这样的规律，既有自身上下文信息，又能和其他词联系起来。目标函数：
    $$F(w_i, w_j, w_k) = \frac{P_{ik}}{P_{jk}}$$  

>* 考点：Glove和word2vec各自有啥优缺点？
    1，Word2vec面向局部特征，基于滑动窗口，而Glove综合了全局语料。  
    2，word2vec可以增量学习，而Glove是由固定语料计算的共现矩阵。

>* 考点：word2vec等方法构建词向量有什么问题？预训练语言模型的优势是什么？
    1， word embedding等都是独立于上下文的，也就是说无论下有任务是什么，输入的word embedding始终是固定的，这就无法解决一词多义，以及在不同语境下有不同表现的需求  
    2，ELMo，GPT以及BERT等都是一次多义以及能够适配下游任务，通过预训练和finetune两个阶段来构造context-dependence的词的表示。

>* 考点：ELMo预训练语言模型网络结构？
    1，ELMo使用双向语言模型来进行预训练，用两个分开的双层LSTM作为encoder，第一层学到的是句法信息，第二层学到的是语义信息。在面对具体下游任务时，首先固定biLM的参数的到一个词的表示，再与上下文无关的词表示（Word2vec，或者charCNN获得的表示）拼接作为模型输入，在反向传播finetune所有参数。

>* 考点：GPT与ELMo的区别？
    ELMo使用LSTM作为编码器，而GPT开始用的是编码能力更强的transformer

>* 考点：对于Self-attention的理解？与普通attention的区别  
    如何学习一个更好的序列表示RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；attention的思路最为粗暴，它一步到位获得了全局信息：
    $$y_t = f(x_t, A, B) $$
其中 $A$，$B$ 是另一个序列（矩阵）。如果 $A = B = X$, 那么就称为Self-attention, 它的意思是直接将$x_t$与原来的每个词进行比较，最后算出$y_t$

>* 考点：multi-head attention 与self-attention的联系？
    这个是google提出的新概念，是attention机制的完善，只是把 $Q$，$K$，$V$通过矩阵映射一下，然后再做Attention，把这个过程重复做 $h$ 次 结果拼接起来就行了

>* 考点：transformer的网络结构
    1，架构上也是一种encode-decode的架构
    2，每个block单元都包含自注意力计算模块，前馈网络，残差模块（Add&Norm，恒等映射的残差连接）

>* 考点：Transformer的残差连接和归一化有何作用？
    1，深层网络会存在梯度消失/爆炸的情况，可以通过Normalization等方式解决，是的模型能够收敛。
    2，在模型能够收敛的情况下，网络越深，模型的准确率越低，同时，模型的准确率先达到饱和然后迅速下降。这个情况我们称之为网络退化。借助ResNet，我们能够有效训练出更深的网络模型（可以超过1000层），使得网络表现不亚于浅层网络。

>* 考点：BERT和GPT的区别？
    1，训练数据不同，GPT使用BooksCorpus（800M words）；BERT是BooksCorpus（800M words）加Wikipedia（2500Mwords）
    2，GPT在训练时没有【CLS】和【SEP】，在下游任务时才有
    3，GPT在finetuning时加入LM的Loss，而Bert是完全使用任务特定目标函数。
    4，GPT的lr在两个1阶段保持一致，Bert认为特定任务的lr效果更好
    5，Bert用mask来实现了双向语言模型，非常巧妙
    6，预训练除了语言模型，还加入了next Sentence prediction，试图学习更高层面的语言相关性。

>* 考点：BERT中的问题和改进思路  
    在实际使用中Q，K，V一般具有相同的维度特征（即hidden—size）比如BERT Base里面是 $d$ = 768, $h$ = 12, 映射矩阵为
    $$W \in R^{d \times (d/h)} $$  
    也就是说每个Attention head里面是将$d$映射到$d/h$,然后再进行Attention计算，输出也是$d/h$维，最后把$h$个$d/h$维的结果拼接起来，得到一个$d$维的输出。这里的$d/h$通常称为head_size  
    在Attention中  
    $$P = softmax (\frac{QK^{T}}{\sqrt{d_k}})$$  
    可以将P看成一个二元联合分布eg.$N^2$，但是我们的参数量只有$n\times (d/h)$ 这就导致了低秩瓶颈（Low-Rank Bottleneck），解决方法是增大模型的key_size也就是Q，K的尺寸。

>* 考点：相对于NNLM，Word2vec的改进都有哪些
>* 考点：哈夫曼树的构建方法，在NLP中有啥应用
>* 考点：分层softmax和负采样的原理和复杂度
>* 考点：负采样的具体实现方法
>* 考点：怎样评估词向量的质量
>* 考点：选出当前query和100万个key词向量相似度的TopK，复杂度尽可能低（faiss）
>* 考点：预训练模型word2vec-glove-ELMo-GPT-BERT-others的演进，每个模型分别解决什么问题

Transformer
>* 考点：为什么Transformer可以实现乱序语言模型，怎么实现的
>* 考点：什么是乱序语言模型
>* 考点：attention矩阵的mask方式与各种预训练方案的关系
>* 考点：直接利用预训练bert模型来做seq2seq任务
>* 考点：为什么要用多头（多个空间学习多种pattern，降低注意力学习的风险）
>* 考点：为什么Q和K的映射矩阵不相同（关系对称，容易得到单位矩阵）
>* 考点：为什么注意力权重要除（防止梯度消失）
>* 考点：为什么用乘性注意力不用加性注意力（乘性计算更小？）
>* 考点：为什么FFN模块（增加模型的非线性能力）
>* 考点：Transformer和BERT的位置编码有啥区别（三角函数式和可学习向量）
>* 考点：残差结构及意义（防止梯度消失和网络退化）
>* 考点：哪个block中更耗时，哪个更占显存（序列短的时候FFN耗时，长的时候MHA耗时；FFN更占显存）
>* 考点：Transformer的layerNorm有哪些（post-norm 和 pre-norm）
>* 考点：Transformer加速（NAR，知识蒸馏，剪枝，动态退出，稀疏注意力，线性注意力）
>* 考点：attention瓶颈（low rank，talking-head）

## 算法重点考点

一般：要求懂原理，应用
重点：手撸
* 最短路算法（一般）：Dijkstra / Floyd / SPFA
* 拓扑排序算法（一定会考）：Topological Sorting
* Morris算法（一般）：O(1)额外空间前序遍历
* 贪心法（重要）：greedy
* Manacher算法（一般）：求最长回文子串
* KMP算法（一般）：strstr / indexOf
* 最小生成树算法（重要）：Minimun Spanning Tree
* 二分法（一定会考）：Binary Search
* 分治法（重要）：Divede & Conquer
* 网络流算法（一般）：Network Flow
* 希尔排序（一般）：Shell Sort
* 动态规划（一定会考）：Dynamic Programming
* 线段树（一般）：Segment Tree
* 平衡排序二叉树（一般）：Red-black Tree
* 字典树（重要）：Trie
* 并查集（重要）：Union Find
* 跳跃表（一般）：Skip List
* 哈希表（一定会考）：Hash Table
* 堆（重要）：Heap
* KD树（一般）：KD-Tree
* B树/B+树（一般）：B-Tree / B+ Tree
* 二叉查找树（一定会考）：Binary Search Tree

