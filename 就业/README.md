## 就业方向

### 岗位方向

* 智能对话
    * 智能客服：京东，淘宝在线客服；快递售后机器人；政企只能外呼；海底捞智能预约机器人；网上商城售后机器人
    * 智能音箱：小度音响；天猫精灵；小爱同学
    * 智能硬件：车载导航；siri

* 智能搜索
    * 搜索引擎搜索：百度搜索；知乎搜索
    * 短视频搜索：抖音搜索；短视频搜索
    * 网上商城搜索：天猫搜索；京东搜索
    * APP搜索：美团搜索

* 智能推荐
    * 短视频推荐：抖音推荐
    * 网上商城推荐：天猫推荐
    * APP推荐：美团推荐
    * 企业营销推荐：银行理财产品营销推荐

* 知识图谱
    * 通用知识图谱：搜索引擎
    * 领域图谱：问答系统

* 语义计算
    * 基础语义表示，预训练

* 多模态
    * 多模态对话问答
    * 多模态搜索
    * 多模态推荐

* 篇章理解
    * 篇章问答

* 数据挖掘
    * 基础数据挖掘，数据分析

* 机器翻译

### 互联网公司

* 基础nlp部门：对话系统；语义计算/语义理解；知识图谱；篇章理解；机器翻译
* ToC业务：搜索；推荐；文本挖掘；视频理解
* ToB业务：云平台；政企业务

### 行业公司

* 金融：营销推荐；客服机器人
* 政企：数据挖掘；质量审核；文本理解
* 教育：作业批改；智能翻译

## 知识点梳理

### 机器学习知识点

| 知识点       | 考点内容                                                   | 难易程度 | 备注 |
|--------------|------------------------------------------------------------|----------|------|
| 线性回归     | 1，能解决什么问题；2，过拟合，欠拟合如何解决               | 一般     |      |
| 逻辑回归     | 1，什么是Sigmoid函数；2，逻辑回归有什么优点以及应用        | 一般     |      |
| 决策树       | 1，ID3算法；2，分类决策树和回归决策树的区别                | 一般     |      |
| 随机森林     | 1，Bagging思想；2，随机森林有什么优缺点                    | 一般     |      |
| XGBoost      | 1，XGBoost和GBDT的区别                                     | 中等     |      |
| SVM          | 1，SVM原理；2，LR和SVM的联系与区别                         | 重点     |      |
| 贝叶斯网络   | 1，贝叶斯网络原理；2，贝叶斯网络；3生成模型与判别模型区别  | 重点     |      |
| 马尔科夫     | 马尔科夫网络，马尔科夫模型，马尔科夫过程，贝叶斯网络的区别 | 重点     |      |
| 主题模型     | 1，LDA模型；2，LDA的topic数如何确定                        | 一般     |      |
| 最大期望算法 | 1，极大似然函数的求解步骤；2，采用EM算法求解的模型有哪些   | 中等     |      |
| 聚类         | 1，什么是无监督算法；2，KNN与K-means有什么区别             | 中等     |      |
| KNN          | 1，什么是KNN；2，K值如何选                                 | 一般     |      |


### 深度学习

| 知识点   | 考点内容                                                        | 难易程度 | 备注 |
|----------|-----------------------------------------------------------------|----------|------|
| CNN      | 1，CNN如何卷积；2，什么是池化；3，CNN优缺点                     | 中等     |      |
| RNN      | 1，RNN结构；2，CNN与RNN区别；3，为什么RNN训练的时候Loss波动很大 | 中等     |      |
| GRU      | 1，门控单元都包含什么                                           | 一般     |      |
| LSTM     | 1，为什么会出现梯度消失，梯度爆炸，现象是什么，如何解决         | 重点     |      |
| 迁移学习 | 1，什么是迁移学习；有哪些方法                                   | 中等     |      |
| 强化学习 | 1，什么是强化学习，有哪些应用场景                               | 中等     |      |
| 集成学习 | 1，集成学习的好处                                               | 重点     |      |


### NLP基础任务

| 知识点   | 考点内容                                         | 难易程度 | 备注 |
|----------|--------------------------------------------------|----------|------|
| 语义表示 | 1，语义表示的演化过程                            | 重点     |      |
| 词性标注 | 1，词性标注的任务是什么，有哪些方法              | 一般     |      |
| 语义相似 | 1，语义相似模型如何构建；2，什么是负采样         | 重点     |      |
| 对话问答 | 1，对话问答的方法一般有哪些；2，对话中有哪些问题 | 重点     |      |
| 信息抽取 | 1，信息抽取任务是什么；2，如何做                 | 重点     |      |
| 图谱问答 | 1，图谱问答优势；2，如何构建图谱问答             | 重点     |      |
| 语义解析 | 1，语义解析任务是什么；2.常用方法有哪些          | 重点     |      |



## NLP备考重点

### 语言模型相关考点
    * 语言模型基础：什么是语言模型；语言模型发展史；语言模型结构；预训练语言模型  
    * 模型优化改进：不同语言模型优缺点；模型优化方法；模型改进趋势  
    * 模型训练问题：模型压缩；模型优化  

* 语言模型基础介绍
  * 什么是语言模型：语言建模的目的就是构建该自然语言处理中词序列的分布，然后用于评估某个词序列的概率。如果给定的词序列符合语用习惯则给出高概率，否则给出低概率
  * 语言模型基础是马尔科夫假设
  * 语言模型计算采用链式法则，单个词序列的概率被分解为序列中各个词的条件概率的乘积，而每个词的条件概率为给定其上下文时改词出现的概率。


$$ p(S) = \prod^{T}_{i=1} p(w_i | w_1,...,w_{i-1}) $$


>* 考点：ngram参数估计  
    Unigram: $2 \times 10^5$  
    Bigram: $(2\times10^5)^2=4\times10^{10}$  
    Trigram: $(2\times10^5)^3=8\times10^{15}$  
    4-gram: $(2\times10^5)^4=16\times10^{20}$  

>* 考点：如何评估语言模型的好坏
1，困惑度（preplexity）：困惑度定义为测试集的概率的倒数，并用单词数做归一化

>* 考点：数据稀疏问题如何解决（单词出现次数少）
大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题  
可用加一平滑法（拉普拉斯定律）解决
$$p_{Add-1}(w_t) = \frac{count(w_t)+1}{T+|V|}$$

>* 考点：极大似然估计 概念

>* 考点：手推BP

>* 考点：神经网络语音模型相较于ngram优点  
2003年bangio提出神经网络语言模型NNLM，提出了词向量的概念，代替了ngram所使用的离散变量而采用连续变量来进行单词的分布式表示，解决了维度灾难和数据稀疏性的问题，同时通过词向量可获得词之间的相似性。

>* 考点：给出词表大小的模型参数量估计
李沐视频

>* 考点：激活函数都有哪些，分别优缺点

>* 考点：Wordvec有两种算法？算法区别是什么？
Skip-garam和CBOW，他们最大的区别是前者是通过中心词预测周边词，后者是用周边词来预测中心词

>* 考点：word2vec为什么能学习到每个单词的向量表示

>* 考点：CBOW模型有哪些问题　　
CBOW模型是用上下文 $X$ 来预测中间词 $Y$ ,那么其输出层（输出层1 * $V$ 的向量）有 $V$ 个神经元，我们对这 $V$ 个神经元一开始是等同对待的，但是如果 $V$ 的数值非常大会导致效率过低，计算量过大。

>* 考点：如何解决CBOW梯度爆炸的问题  
使用哈夫曼树来代替从隐藏层输出softmax层的映射，（哈夫曼树的性质，越靠近根节点的地方，词频（权重）越高，我们可以更快的使用这个词，相反的越靠近叶节点的地方，词频率越低，这两就提高了训练效果）。在哈夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着哈夫曼树一步步完成的，因此这种softmax取名为hierarchical softmax，即分层softmax。

>* 考点：什么是hierarchical softmax？  
根据标签（label）和频率建立哈夫曼树；（label出现的频率越高，huffman树的路径越短，huffman树中的每一个叶子结点代表一个label）

>* 考点：hierarchical softmax代替softmax后复杂度变成了多少
复杂度从 $O(N^2)$ 变成了 $O(\log N)$

>* 考点：fasttext主要特点  
fasttext最早其实是一个文本分类算法，后续加了一些改进来训练词向量，主要有：
    * fastext在输入时对每个词加入了n-gram特征，在输出时使用分层softmax加速训练。
    * fasttext将整篇文章的词向量求平均作为输入得到文档向量，用文本分类做有监督训练，对输出进行softmax回归，词向量为副产品。
    * fasttext也可以无监督训练词向量，与CBOW非常相似。

>* 考点：什么是负采样
>* 考点：负采样有什么作用
>* 考点：如何更好得选择负样本，即负采样？
    如skipgram 1，一个办法是对中间的这些词进行采样，即候选的目标词，根据其在语料中的经验频率（empirical frequency）进行采样，就是通过词出现的频率进行采样，但是这样会导致在like，the，of，and诸如此类的词上有很好高的频率。
    2，另一个极端就是用1除以词汇表总词数，即 $\frac{1}{|V|}$, 均匀且随机地抽取负样本，这对于英文单词的分布是非常没有代表性的。
    3，论文的作者Mikolov等人根据经验，他们发现这个经验值的效果最好，它位于这两个极端的采样方法之间，既不用经验频率，也就是实际观察到的英文文本的分布，也不用均匀分布，他们采用以下方式:
    $$p(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\Sigma^{10,000}_{j=1}f(w_j)^{\frac{3}{4}}} $$
    上式的 $f(w_i)$ 是观测到的在语料库中的某个英文词的词频，通过 $\frac{3}{4}$ 次方的计算，使其处于完全独立的分布和训练集的观测分布两个极端之间。Andrew并不确定这是否有理论证明，但是很多研究者现在使用这个方法似乎效果也不错

>* 考点：Glove训练过程？
    1，构建共现矩阵：Glove指定特定大小的上下文窗口，通过滑动该窗口统计共现矩阵 $X(|V| \times |V|)$,$X_{ij}$ 表示中心词i与上下文j的共现次数。同时还定义了衰减函数，令距离为d的两个词在计数时乘以 $\frac{1}{d}$  
    2，确定近似目标：作者发现可以用概率之比来建模共现关系，定义条件概率 $P$ :
    $$P_{ij} = P(j|i) = \frac{X_{ij}}{X_i}$$  
    3，表示词 $j$ 出现在 $i$ 上下文的概率。而用词 $k$ 出现在 $i$ 的上下文与它出现在 $j$ 上下文的概率之比：
    $$ratio_{ijk} = \frac{P_{ik}}{P_{jk}}$$  
    来表示 $i$, $j$, $k$ 三个词的共现关系。当 $i$, $k$ 和 $j$, $k$相关程度近似时，该比率趋近于1；$i$，$k$ 相关度大于$j$，$k$相关度时该比率值较大，反之较小。Glove的目标就是使学习到的词向量满足这样的规律，既有自身上下文信息，又能和其他词联系起来。目标函数：
    $$F(w_i, w_j, w_k) = \frac{P_{ik}}{P_{jk}}$$  

>* 考点：Glove和word2vec各自有啥优缺点？
    1，Word2vec面向局部特征，基于滑动窗口，而Glove综合了全局语料。
    2，word2vec可以增量学习，而Glove是由固定语料计算的共现矩阵。







